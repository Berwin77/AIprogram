{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "import pyhocon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device 0 TITAN X (Pascal)\n",
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.dataSet = 'cora'\n",
    "        self.agg_func = 'MEAN'\n",
    "        self.epochs = 2\n",
    "        self.b_sz = 20\n",
    "        self.seed = 824\n",
    "        self.cuda = 'use CUDA'\n",
    "        self.gcn = 'store_true'\n",
    "        self.learn_method = 'unsup'\n",
    "        self.unsup_loss = 'normal'\n",
    "        self.max_vali_f1 = 0\n",
    "        self.name = 'debug'\n",
    "        self.config = './../data/experiments.conf'\n",
    "\n",
    "args = Config()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "    else:\n",
    "        device_id = torch.cuda.current_device()\n",
    "        print('using device', device_id, torch.cuda.get_device_name(device_id))\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print('DEVICE:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据集介绍为： https://www.cnblogs.com/popodynasty/p/14975945.html\n",
    "class DataCenter(object):\n",
    "    def __init__(self,config):\n",
    "        super(DataCenter,self).__init__()\n",
    "        self.config = config\n",
    "    def load_dataSet(self,dataSet='cora'):\n",
    "        cora_content_file = self.config[\"file_path.cora_content\"]\n",
    "        cora_cite_file = self.config[\"file_path.cora_cite\"]\n",
    "        feat_data = []\n",
    "        labels = []\n",
    "        node_map = {}\n",
    "        label_map = {}\n",
    "        \n",
    "        with open(cora_content_file) as fp:                              # 打开节点特性信息文件\n",
    "            for i, line in enumerate(fp):\n",
    "                info = line.strip().split()                              \n",
    "                feat_data.append([float(x) for x in info[1:-1]])         # 节点特征信息\n",
    "                node_map[info[0]] = i                                    # 节点的index  值->ID\n",
    "                if not info[-1] in label_map:                            # 判断标签是否已存在\n",
    "                    label_map[info[-1]] = len(label_map)                 # 标签的index  值->ID\n",
    "                labels.append(label_map[info[-1]])                       # 节点对应的标签信息\n",
    "        \n",
    "        # 原始版  读取已有embedding\n",
    "        feat_data = np.asarray(feat_data)                                # 转换成numpy格式\n",
    "\n",
    "        \n",
    "        labels = np.asarray(labels, dtype=np.int64)                      # 转换成numpy格式\n",
    "        adj_lists = defaultdict(set)  \n",
    "        \n",
    "        with open(cora_cite_file) as fp:\n",
    "            for i, line in enumerate(fp):\n",
    "                info = line.strip().split()\n",
    "                assert len(info) == 2\n",
    "                paper1 = node_map[info[0]]\n",
    "                paper2 = node_map[info[1]]\n",
    "                # 得到的字典可以查找每一个点的邻居节点\n",
    "                adj_lists[paper1].add(paper2)\n",
    "                adj_lists[paper2].add(paper1)\n",
    "        assert len(feat_data) == len(labels) == len(adj_lists)\n",
    "        \n",
    "        # 把数据分为 训练，验证，测试集\n",
    "        test_indexs,val_indexs, train_indexs = self._split_data(feat_data.shape[0])\n",
    "\n",
    "\n",
    "        setattr(self, dataSet+\"_test\", test_indexs)\n",
    "        setattr(self,dataSet+\"_val\", val_indexs)\n",
    "        setattr(self,dataSet+\"_train\", train_indexs)\n",
    "        \n",
    "        setattr(self,dataSet+\"_feats\", feat_data)\n",
    "        setattr(self,dataSet+\"_labels\", labels)\n",
    "        setattr(self, dataSet+\"_adj_lists\", adj_lists)\n",
    "    \n",
    "    def _split_data(self,num_nodes,test_split =3, val_split=6):\n",
    "        # 随机打乱\n",
    "        rand_indices = np.random.permutation(num_nodes)\n",
    "        test_size = num_nodes // test_split\n",
    "        val_size = num_nodes // val_split\n",
    "        train_size = num_nodes -(test_size + val_size)\n",
    "        test_indexs = rand_indices[:test_size]\n",
    "        val_indexs = rand_indices[test_size:(test_size+val_size)]\n",
    "        train_indexs = rand_indices[(test_size+val_size):]\n",
    "        \n",
    "        return test_indexs, val_indexs, train_indexs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageLayer(nn.Module):\n",
    "    def __init__(self, input_size, out_size,gcn=False):\n",
    "        super(SageLayer,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "        self.gcn = gcn\n",
    "        # 创建weight\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gcn else 2*self.input_size))\n",
    "        # 初始化参数\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "#             print(\"param=====\",param)\n",
    "            nn.init.xavier_uniform_(param)\n",
    "    def forward(self, self_feats, aggregate_feats,neghs=None):\n",
    "        if not self.gcn:\n",
    "            # concat自己信息和邻居信息\n",
    "            combined = torch.cat([self_feats,aggregate_feats],dim=1)\n",
    "        else:\n",
    "            combined = aggregate_feats\n",
    "        # mm() 代表 matmul()  .t() 为 转置\n",
    "        combined = F.relu(self.weight.mm(combined.t())).t()\n",
    "        return combined \n",
    "\n",
    "class GraphSage(nn.Module):\n",
    "    def __init__(self, num_layers,input_size,out_size, raw_features, adj_lists, device,gcn=False,agg_func=\"MEAN\"):\n",
    "        super(GraphSage,self).__init__()\n",
    "        # 输入尺寸 1433\n",
    "        self.input_size = input_size\n",
    "        # 输出尺寸 128\n",
    "        self.out_size = out_size\n",
    "        # 聚合层数 2\n",
    "        self.num_layers = num_layers\n",
    "        # 是否使用GCN \n",
    "        self.gcn = gcn\n",
    "        # 使用训练设备\n",
    "        self.device = device\n",
    "        # 聚合函数\n",
    "        self.agg_func = agg_func\n",
    "        # 节点特征\n",
    "        self.raw_features = raw_features\n",
    "        # 边\n",
    "        self.adj_lists = adj_lists\n",
    "        \n",
    "        for index in range(1,num_layers+1):\n",
    "            layer_size = out_size if index!=1 else input_size\n",
    "            setattr(self, 'sage_layer'+str(index), SageLayer(layer_size,out_size,gcn=self.gcn))\n",
    "    \n",
    "    def forward(self,nodes_batch):\n",
    "        # 把当前训练的节点转换成list\n",
    "        lower_layer_nodes = list(nodes_batch)\n",
    "        # 放入训练节点\n",
    "        nodes_batch_layers = [(lower_layer_nodes,)]\n",
    "        # 遍历每一次聚合，获得neighbors\n",
    "        for i in range(self.num_layers):\n",
    "            # batch涉及到的所有节点:Set(本身+邻居), dict(节点编号->当前字典中顺序index), [合并所有节点]\n",
    "            lower_samp_neighs,lower_layer_nodes_dict, lower_layer_nodes = self._get_unique_neighs_list(lower_layer_nodes)\n",
    "              \n",
    "            #[([合并所有节点], [Set(本身+邻居)] , dict(节点编号->当前字典中顺序index)),([batch节点]),] \n",
    "            nodes_batch_layers.insert(0,(lower_layer_nodes,lower_samp_neighs,lower_layer_nodes_dict))\n",
    "        \n",
    "        assert len(nodes_batch_layers) == self.num_layers +1\n",
    "        \n",
    "        pre_hidden_embs = self.raw_features\n",
    "        \n",
    "        for index in range(1,self.num_layers+1):\n",
    "            # 聚合自己和邻居的节点\n",
    "            nb = nodes_batch_layers[index][0]\n",
    "            #[([合并所有节点], [Set(本身+邻居)] , dict(节点编号->当前字典中顺序index)),([batch节点]),] \n",
    "            pre_neighs = nodes_batch_layers[index-1]\n",
    "            # 聚合函数。聚合的节点， 节点特征，集合节点邻居信息\n",
    "            aggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs)\n",
    "            \n",
    "            sage_layer = getattr(self,  \"sage_layer\"+str(index))\n",
    "            \n",
    "            if index>1:\n",
    "                # 第一层的batch节点，没有进行转换\n",
    "                nb = self._nodes_map(nb,pre_hidden_embs, pre_neighs)\n",
    "            # 进入SageLayer。weight*concat(node,neighbors)\n",
    "            cur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb], aggregate_feats=aggregate_feats)\n",
    "            \n",
    "            pre_hidden_embs = cur_hidden_embs\n",
    "        return pre_hidden_embs\n",
    "            \n",
    "                \n",
    "            \n",
    "        \n",
    "    def _nodes_map(self, nodes,hidden_embs, neighs):\n",
    "        layer_nodes, samp_neighs, layer_nodes_dict = neighs\n",
    "        assert len(samp_neighs) == len(nodes)\n",
    "        # 得到上一层dict中的节点index\n",
    "        index = [layer_nodes_dict[x] for x in nodes]\n",
    "        return index\n",
    "        \n",
    "    def aggregate(self,nodes,pre_hidden_embs,pre_neighs,num_sample=10):\n",
    "        #[([合并所有节点], [Set(本身+邻居)] , dict(节点编号->当前字典中顺序index)),([batch节点]),] \n",
    "        unique_nodes_list, samp_neighs, unique_nodes = pre_neighs\n",
    "        assert len(nodes) == len(samp_neighs)\n",
    "        # 判断是否包含本身\n",
    "        indicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))]\n",
    "        assert (False not in indicator)\n",
    "        \n",
    "        if not self.gcn:\n",
    "            # 中心节点删除\n",
    "            samp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]\n",
    "        if len(pre_hidden_embs) == len(unique_nodes):\n",
    "            embed_matrix = pre_hidden_embs\n",
    "        else:\n",
    "            embed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)]\n",
    "        # (本层节点数，总邻居节点数)\n",
    "        mask = torch.zeros(len(samp_neighs),len(unique_nodes))  \n",
    "        # 每个node 邻接节点对应的列 index\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh ]\n",
    "        # 每个node 对应的行 index \n",
    "        row_indices = [i for i in range(len(samp_neighs)) for _ in range(len(samp_neighs[i]))]\n",
    "        # 构建邻接矩阵\n",
    "        mask[row_indices,column_indices] = 1\n",
    "        \n",
    "        if(self.agg_func==\"MEAN\"):\n",
    "            # 按行求和，保持和输入一个维度\n",
    "            num_neigh = mask.sum(1,keepdim=True)\n",
    "            # 归一化操作\n",
    "            mask = mask.div(num_neigh).to(embed_matrix.device)\n",
    "            # 矩阵相乘，相当于聚合周围邻接信息求和\n",
    "            aggregate_feats = mask.mm(embed_matrix)\n",
    "        elif self.agg_func == \"MAX\":\n",
    "            # 不为0的行\n",
    "            indexs = [x.nonzero() for x in mask==1]\n",
    "            aggregate_feats = []\n",
    "            for feat in [embed_matrix[x.squeeze()] for x in indexs]:\n",
    "                if len(feat.size())==1:\n",
    "                    # view 相当于tf的reshape\n",
    "                    aggregate_feats.append(feat.view(1, -1))\n",
    "                else:\n",
    "                    aggregate_feats.append(torch.max(feat,0)[0].view(1,-1))\n",
    "            aggregate_feats = torch.cat(aggregate_feats,0)\n",
    "        \n",
    "        return aggregate_feats\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _get_unique_neighs_list(self,nodes,num_sample=10):\n",
    "        _set = set\n",
    "         # self.adj_lists边矩阵，获取节点的邻居\n",
    "        to_neighs = [self.adj_lists[int(node)] for node in nodes]\n",
    "        \n",
    "        if not num_sample is None:\n",
    "            _sample = random.sample\n",
    "            # 对邻居节点进行采样，如果大于邻居数据，则进行采样\n",
    "            samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh)>=num_sample else to_neigh \n",
    "                           for to_neigh in to_neighs]\n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "        # 两个set  | 相当于取并集 与union()效果相同\n",
    "        # 加入本身节点\n",
    "        samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "        # 这个batch涉及到的所有节点\n",
    "        _unique_nodes_list = list(set.union(*samp_neighs))\n",
    "        # 建立编号\n",
    "        i = list(range(len(_unique_nodes_list)))\n",
    "        # 节点编号->当前字典中顺序index\n",
    "        unique_nodes = dict(list(zip(_unique_nodes_list, i)))\n",
    "        # 聚合自己和邻居节点，点的dict，batch涉及到的所有节点\n",
    "        return samp_neighs, unique_nodes, _unique_nodes_list\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module): \n",
    "    # 把GraphSAGE的输出链接全连接层每个节点映射到7维\n",
    "    def __init__(self,emb_size,num_classes):\n",
    "        super(Classification,self).__init__()\n",
    "        self.layer = nn.Sequential(nn.Linear(emb_size,num_classes))\n",
    "        self.init_params()\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if len(param.size())==2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "    def forward(self, embeds):\n",
    "        logists = torch.log_softmax(self.layer(embeds),1)\n",
    "        return logists\n",
    "\n",
    "    \n",
    "class UnsupervisedLoss(object):\n",
    "    # 无监督loss\n",
    "    def __init__(self,adj_lists,train_nodes,device):\n",
    "        super(UnsupervisedLoss,self).__init__()\n",
    "        self.Q = 10\n",
    "        self.N_WALKS = 6\n",
    "        self.WALK_LEN = 1\n",
    "        self.N_WALK_LEN = 5\n",
    "        self.MARGIN = 3\n",
    "        self.adj_lists = adj_lists\n",
    "        # train_nodes 代表所有训练节点\n",
    "        self.train_nodes = train_nodes\n",
    "        self.device = device\n",
    "        \n",
    "        self.target_nodes = None\n",
    "        self.positive_pairs = []\n",
    "        self.negtive_pairs = []\n",
    "        self.node_positive_pairs = {}\n",
    "        self.node_negtive_pairs = {}\n",
    "        self.unique_nodes_batch = []\n",
    "        \n",
    "    def get_loss_sage(self, embeddings, nodes):\n",
    "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
    "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
    "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
    "\n",
    "        nodes_score = []\n",
    "        assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n",
    "        for node in self.node_positive_pairs:\n",
    "            pps = self.node_positive_pairs[node]\n",
    "            nps = self.node_negtive_pairs[node]\n",
    "            if len(pps)==0 or len(nps)==0:\n",
    "                continue\n",
    "            # Q * Exception(negative score)\n",
    "            indexs = [list(x) for x in zip(*nps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            neg_score = F.cosine_similarity(embeddings[node_indexs],embeddings[neighb_indexs])\n",
    "            neg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)),0)\n",
    "\n",
    "\n",
    "#             # multiple positive score\n",
    "            indexs = [list(x) for x in zip(*pps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            pos_score = torch.log(torch.sigmoid(pos_score))\n",
    "    \n",
    "    \n",
    "            nodes_score.append(torch.mean(-pos_score-neg_score).view(1,-1))\n",
    "        loss = torch.mean(torch.cat(nodes_score,0))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def get_loss_margin(self,embeddings,nodes):\n",
    "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
    "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
    "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
    "\n",
    "        nodes_score = []\n",
    "        assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n",
    "        for node in self.node_positive_pairs:\n",
    "            pps = self.node_positive_pairs[node]\n",
    "            nps = self.node_negtive_pairs[node]\n",
    "            if len(pps) == 0 or len(nps) == 0:\n",
    "                continue\n",
    "\n",
    "            indexs = [list(x) for x in zip(*pps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "#             pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            pos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)\n",
    "\n",
    "            indexs = [list(x) for x in zip(*nps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            heg_score,_ = torch.max(torch.log(torch.sigmoid(neg_score)),0)\n",
    "\n",
    "            nodes_score.append(torch.max(torch.tensor(0.0).to(self.device), neg_score-pos_score+self.MARGIN).VIEW(1,-1))\n",
    "\n",
    "\n",
    "        loss = torch.mean(torch.cat(nodes_score,0),0)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def extend_nodes(self, nodes,num_neg=6):\n",
    "        self.positive_pairs = []\n",
    "        self.node_positive_pairs = {}\n",
    "        self.negtive_pairs = []\n",
    "        self.node_negtive_pairs = {}\n",
    "\n",
    "        self.target_nodes = nodes\n",
    "        self.get_positive_nodes(nodes)\n",
    "#         print(\"positive_pairs\", self.positive_pairs)\n",
    "        self.get_negtive_nodes(nodes, num_neg)\n",
    "#         print(\"negtive_pairs\", self.negtive_pairs)\n",
    "\n",
    "        self.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x])| set([i for x in self.negtive_pairs for i in x]))\n",
    "        assert set(self.target_nodes) < set(self.unique_nodes_batch)\n",
    "        return self.unique_nodes_batch\n",
    "\n",
    "    def get_positive_nodes(self,nodes):\n",
    "        return self._run_random_walks(nodes)\n",
    "\n",
    "    def get_negtive_nodes(self, nodes, num_neg):\n",
    "        for node in nodes:\n",
    "            neighbors = set([node])\n",
    "            frontier = set([node])\n",
    "            # 下方循环就是得到 当前node 0~N_WALK_LEN 度的所有节点set集合：neighbors  \n",
    "            for i in range(self.N_WALK_LEN):\n",
    "                current = set()\n",
    "                for outer in frontier:\n",
    "                    # 取并集 current 为outer所有相邻节点 set集合\n",
    "                    current |= self.adj_lists[int(outer)]\n",
    "                # 表示node i+1度的关联节点\n",
    "                frontier = current - neighbors\n",
    "                # neighbors 添加上 current： 表示node 从0~i+1 度 所有关联的节点\n",
    "                neighbors |= current\n",
    "\n",
    "            # train_nodes 代表所有训练节点\n",
    "            # 除去neighbors 后进行采样\n",
    "            far_nodes = set(self.train_nodes) - neighbors\n",
    "            #  num_neg为采样数量 这里默认设置为 6 \n",
    "            neg_samples = random.sample(far_nodes,num_neg) if num_neg<len(far_nodes) else far_nodes \n",
    "\n",
    "            self.negtive_pairs.extend([node, neg_node] for neg_node in neg_samples)\n",
    "            self.node_negtive_pairs[node] = [(node, neg_node) for neg_node in neg_samples]\n",
    "        return self.negtive_pairs\n",
    "\n",
    "    def _run_random_walks(self, nodes):\n",
    "        for node in nodes:\n",
    "            if(len(self.adj_lists[int(node)])) ==0 :\n",
    "                continue\n",
    "            cur_pairs = []\n",
    "            for i in range(self.N_WALKS):\n",
    "                curr_node = node\n",
    "                for j in range(self.WALK_LEN):\n",
    "                    neighs = self.adj_lists[int(curr_node)]\n",
    "                    next_node = random.choice(list(neighs))\n",
    "                    # 共线是无用的\n",
    "                    if next_node != node and next_node in self.train_nodes:\n",
    "                        self.positive_pairs.append((node, next_node))\n",
    "                        cur_pairs.append((node,next_node))\n",
    "                    curr_node = next_node\n",
    "            self.node_positive_pairs[node] = cur_pairs\n",
    "        return self.positive_pairs\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataCenter, ds, graphSage, classification, device, max_vali_f1, name, cur_epoch):\n",
    "    test_nodes = getattr(dataCenter, ds+'_test')\n",
    "    val_nodes = getattr(dataCenter, ds+'_val')\n",
    "    labels = getattr(dataCenter, ds+'_labels')\n",
    "    \n",
    "    models = [graphSage, classification]\n",
    "    \n",
    "    params = []\n",
    "    for model in models:\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:\n",
    "                param.requires_grad = False\n",
    "                params.append(param)\n",
    "    \n",
    "    embs = graphSage(val_nodes)\n",
    "    logists = classification(embs)\n",
    "    _,predicts = torch.max(logists,1)\n",
    "    labels_val = labels[val_nodes]\n",
    "    \n",
    "    assert len(labels_val) == len(predicts)\n",
    "    print(\"!!!!!!!\",type(predicts))\n",
    "    print(\"@@@@@@@@@@@\",type(predicts.data))\n",
    "#     print(\"#####\", predicts==predicts.data)\n",
    "    comps = zip(labels_val, predicts.data)\n",
    "    \n",
    "    vali_f1 = f1_score(labels_val, predicts.cpu().data, average=\"micro\")\n",
    "    print(\"Validation F1:\", vali_f1)\n",
    "    \n",
    "    if vali_f1 > max_vali_f1:\n",
    "        max_vali_f1 = vali_f1\n",
    "        embs = graphSage(test_nodes)\n",
    "        logists = classification(embs)\n",
    "        _,predicts = torch.max(logists, 1)\n",
    "        labels_test = labels[test_nodes]\n",
    "        assert len(labels_test) == len(predicts)\n",
    "        comps = zip(labels_test, predicts.data)\n",
    "        \n",
    "        test_f1 = f1_score(labels_test, predicts.cpu().data, average=\"micro\")\n",
    "        print(\"Test F1:\", test_f1)\n",
    "        \n",
    "        for param in params:\n",
    "            param.requires_grad = True\n",
    "        \n",
    "\n",
    "        torch.save(models, '../models/model_best_{}_ep{}_{:.4f}.torch'.format(name, cur_epoch, test_f1))\n",
    "        \n",
    "    for param in params:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    return max_vali_f1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(dataCenter, ds, graphSage, classification, unsupervised_loss, b_sz, unsup_loss, device, learn_method):\n",
    "    \n",
    "    test_nodes = getattr(dataCenter, ds+'_test')\n",
    "    val_nodes = getattr(dataCenter, ds+'_val')\n",
    "    train_nodes = getattr(dataCenter, ds+'_train')\n",
    "    labels = getattr(dataCenter, ds+'_labels')\n",
    "    \n",
    "    if unsup_loss == 'margin':\n",
    "        num_neg = 6\n",
    "    elif unsup_loss == 'normal':\n",
    "        num_neg = 100\n",
    "    else:\n",
    "        print(\"unsup_loss can be only 'margin' or 'normal'.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    \n",
    "    train_nodes = shuffle(train_nodes)\n",
    "    \n",
    "    models = [graphSage, classification]\n",
    "    \n",
    "    params = []\n",
    "    # 初始化模型参数\n",
    "    for model in models:\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:\n",
    "                params.append(param)\n",
    "    \n",
    "    # 梯度优化算法\n",
    "    optimizer = torch.optim.SGD(params, lr=0.7)  \n",
    "    # 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    for model in models:\n",
    "        model.zero_grad()\n",
    "    \n",
    "    # 有多少个batches\n",
    "    batches = math.ceil(len(train_nodes)/b_sz)\n",
    "    \n",
    "    visited_nodes = set()\n",
    "    \n",
    "    for index in range(batches):\n",
    "        nodes_batch = train_nodes[index*b_sz:(index+1)*b_sz]\n",
    "        \n",
    "        # 负采样\n",
    "        nodes_batch = np.asarray(list(unsupervised_loss.extend_nodes(nodes_batch,num_neg=num_neg)))\n",
    "        visited_nodes |= set(nodes_batch)\n",
    "        \n",
    "        # 拿标签\n",
    "        labels_batch = labels[nodes_batch]\n",
    "        \n",
    "        # 整个batch数据输入 graphSage\n",
    "        \n",
    "        embs_batch = graphSage(nodes_batch)\n",
    "        \n",
    "        if learn_method == 'sup':\n",
    "            # 有监督学习\n",
    "            logists = classification(embs_batch)\n",
    "            loss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch],0)\n",
    "            loss_sup /= len(nodes_batch)\n",
    "            loss = loss_sup\n",
    "            \n",
    "        elif learn_method == 'plus_unsup':\n",
    "            # 混合 有监督和无监督\n",
    "            logists = classification(embs_batch)\n",
    "            loss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
    "            loss_sup /= len(nodes_batch)\n",
    "            \n",
    "            # 无监督\n",
    "            if unsup_loss == 'margin':\n",
    "                loss_net = unsupervised_loss.get_loss_margin(embs_batch, nodes_batch)\n",
    "            elif unsup_loss == 'normal':\n",
    "                loss_net = unsupervised_loss.get_loss_sage(embs_batch, nodes_batch)\n",
    "            \n",
    "            loss = loss_sup + loss_net\n",
    "        else:\n",
    "            \n",
    "            # 无监督\n",
    "            if unsup_loss == 'margin':\n",
    "                loss_net = unsupervised_loss.get_loss_margin(embs_batch, nodes_batch)\n",
    "            elif unsup_loss == 'normal':\n",
    "                loss_net = unsupervised_loss.get_loss_sage(embs_batch, nodes_batch)\n",
    "            loss = loss_net\n",
    "            \n",
    "        if (index+1)%20 ==0:\n",
    "            print('Step [{}/{}], Loss: {:.4f}, Dealed Nodes [{}/{}] '.format(index+1, batches, loss.item(), len(visited_nodes), len(train_nodes)))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        if learn_method == 'unsup':\n",
    "            for model in models[:-1]:\n",
    "            # 相当于dropout\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 500)\n",
    "        else:    \n",
    "            for model in models:\n",
    "                # 相当于dropout\n",
    "#                 print(model.parameters)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 500)\n",
    "        # 更新梯度\n",
    "        optimizer.step()                                      \n",
    "        optimizer.zero_grad() \n",
    "        for model in models:\n",
    "            model.zero_grad()\n",
    "        \n",
    "    return graphSage, classification\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~ torch.Size([2708, 1433])\n",
      "GraphSage with Supervised Learning\n",
      "----------------------EPOCH 0-----------------------\n",
      "Step [20/68], Loss: 1.0012, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.3863, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.1952, Dealed Nodes [1355/1355] \n",
      "!!!!!!! <class 'torch.Tensor'>\n",
      "@@@@@@@@@@@ <class 'torch.Tensor'>\n",
      "Validation F1: 0.8869179600886918\n",
      "----------------------EPOCH 1-----------------------\n",
      "Step [20/68], Loss: 0.1496, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.1300, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.1189, Dealed Nodes [1355/1355] \n",
      "!!!!!!! <class 'torch.Tensor'>\n",
      "@@@@@@@@@@@ <class 'torch.Tensor'>\n",
      "Validation F1: 0.8736141906873615\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    # load config file\n",
    "    config = pyhocon.ConfigFactory.parse_file(args.config)\n",
    "    # load data\n",
    "    ds = args.dataSet\n",
    "    \n",
    "    dataCenter = DataCenter(config)\n",
    "    dataCenter.load_dataSet(ds)\n",
    "    features = torch.FloatTensor(getattr(dataCenter, ds+'_feats')).to(device)  \n",
    "    print(\"~~~~~~~~~~~\",features.shape)\n",
    "    graphSage = GraphSage(config['setting.num_layers'], features.size(1), config['setting.hidden_emb_size'], features, getattr(dataCenter, ds+'_adj_lists'), device, gcn=args.gcn, agg_func=args.agg_func)\n",
    "    graphSage.to(device)\n",
    "\n",
    "    num_labels = len(set(getattr(dataCenter, ds+'_labels')))\n",
    "    classification = Classification(config['setting.hidden_emb_size'], num_labels)\n",
    "    classification.to(device)\n",
    "    \n",
    "    unsupervised_loss = UnsupervisedLoss(getattr(dataCenter, ds+'_adj_lists'), getattr(dataCenter, ds+'_train'), device)\n",
    "    \n",
    "    args.learn_method = \"sup\"\n",
    "    \n",
    "    if args.learn_method == 'sup':\n",
    "        print('GraphSage with Supervised Learning')\n",
    "    elif args.learn_method == 'plus_unsup':\n",
    "        print('GraphSage with Supervised Learning plus Net Unsupervised Learning')\n",
    "    else:\n",
    "        print('GraphSage with Net Unsupervised Learning')\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        print('----------------------EPOCH %d-----------------------' % epoch)\n",
    "        graphSage, classification = apply_model(dataCenter, ds, graphSage, classification, unsupervised_loss, args.b_sz, args.unsup_loss,device, args.learn_method)\n",
    "        \n",
    "#         if(epoch+1) %2 ==0 and args.learn_method == \"unsup\":\n",
    "#             classification, args.max_vali_f1 = train_classification(dataCenter, graphSage, classification,ds, device, args.max_vali_f1, args.name)\n",
    "        \n",
    "        if args.learn_method != 'unsup':\n",
    "            args.max_vali_f1 = evaluate(dataCenter, ds, graphSage, classification, device,args.max_vali_f1,args.name, epoch)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "val_indexs = features[1]\n",
    "print(val_indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] [4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "s = [[1,2,3],[4,5,6]]\n",
    "print(*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
