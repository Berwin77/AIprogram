{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Input\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM_Layer(Layer):\n",
    "    def __init__(self, feature_columns,k,w_reg=1e-6, v_reg=1e-6):\n",
    "\n",
    "        super(FM_Layer, self).__init__()\n",
    "        self.sparse_feature_columns = feature_columns\n",
    "        self.index_mapping = []\n",
    "        self.feature_length = 0\n",
    "        for feat in self.sparse_feature_columns:\n",
    "            self.index_mapping.append(self.feature_length)\n",
    "            self.feature_length += feat['feat_num']\n",
    "        self.k = k\n",
    "        self.w_reg = w_reg\n",
    "        self.v_reg = v_reg\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w0 = self.add_weight(name=\"w0\", shape=(1,), initializer=tf.zeros_initializer(), trainable=True)\n",
    "        self.w = self.add_weight(name='w', shape=(self.feature_length,1),\n",
    "                                initializer=tf.random_normal_initializer(),\n",
    "                                regularizer=l2(self.w_reg),trainable=True)\n",
    "        self.V = self.add_weight(name='V', shape= (self.feature_length, self.k), initializer=tf.random_normal_initializer(),\n",
    "                                regularizer=l2(self.v_reg),trainable=True)\n",
    "        \n",
    "    def call(self,inputs,**kwargs):\n",
    "        inputs = inputs + tf.convert_to_tensor(self.index_mapping)\n",
    "        first_order = self.w0 + tf.reduce_sum(tf.nn.embedding_lookup(self.w, inputs), axis=1) #(batch_size,1)\n",
    "        second_inputs = tf.nn.embedding_lookup(self.V,inputs) #(batch_size, 1, embed_dim)\n",
    "        square_sum = tf.square(tf.reduce_sum(second_inputs, axis=1,keepdims=True))  # (batch_size, 1, embed_dim)\n",
    "        sum_square = tf.reduce_sum(tf.square(second_inputs), axis=1,keepdims=True) #(batch_size,1, embed_dim)\n",
    "        second_order = 0.5*tf.reduce_sum(square_sum-sum_square, axis = 2) #(batch_size,1)\n",
    "        outputs = first_order + second_order\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "class FM(Model):\n",
    "    \n",
    "    def __init__(self, feature_columns, k, w_reg=1e-6,v_reg=1e-6):\n",
    "        super(FM, self).__init__()\n",
    "        self.sparse_feature_columns = feature_columns\n",
    "        self.fm = FM_Layer(feature_columns, k, w_reg, v_reg)\n",
    "            \n",
    "    def call(self, inputs, **kwargs):\n",
    "        fm_outputs = self.fm(inputs)\n",
    "        outputs = tf.nn.sigmoid(fm_outputs)\n",
    "        return outputs\n",
    "    def summary(self,**kwargs):\n",
    "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),),dtype=tf.int32)\n",
    "        Model(inputs=sparse_inputs,outputs=self.call(sparse_inputs)).summary()\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'feat_name': 'C1', 'feat_num': 541, 'embed_dim': 8}, {'feat_name': 'C2', 'feat_num': 497, 'embed_dim': 8}, {'feat_name': 'C3', 'feat_num': 43870, 'embed_dim': 8}, {'feat_name': 'C4', 'feat_num': 25184, 'embed_dim': 8}, {'feat_name': 'C5', 'feat_num': 145, 'embed_dim': 8}, {'feat_name': 'C6', 'feat_num': 12, 'embed_dim': 8}, {'feat_name': 'C7', 'feat_num': 7623, 'embed_dim': 8}, {'feat_name': 'C8', 'feat_num': 257, 'embed_dim': 8}, {'feat_name': 'C9', 'feat_num': 3, 'embed_dim': 8}, {'feat_name': 'C10', 'feat_num': 10997, 'embed_dim': 8}, {'feat_name': 'C11', 'feat_num': 3799, 'embed_dim': 8}, {'feat_name': 'C12', 'feat_num': 41312, 'embed_dim': 8}, {'feat_name': 'C13', 'feat_num': 2796, 'embed_dim': 8}, {'feat_name': 'C14', 'feat_num': 26, 'embed_dim': 8}, {'feat_name': 'C15', 'feat_num': 5238, 'embed_dim': 8}, {'feat_name': 'C16', 'feat_num': 34617, 'embed_dim': 8}, {'feat_name': 'C17', 'feat_num': 10, 'embed_dim': 8}, {'feat_name': 'C18', 'feat_num': 2548, 'embed_dim': 8}, {'feat_name': 'C19', 'feat_num': 1303, 'embed_dim': 8}, {'feat_name': 'C20', 'feat_num': 4, 'embed_dim': 8}, {'feat_name': 'C21', 'feat_num': 38618, 'embed_dim': 8}, {'feat_name': 'C22', 'feat_num': 11, 'embed_dim': 8}, {'feat_name': 'C23', 'feat_num': 14, 'embed_dim': 8}, {'feat_name': 'C24', 'feat_num': 12335, 'embed_dim': 8}, {'feat_name': 'C25', 'feat_num': 51, 'embed_dim': 8}, {'feat_name': 'C26', 'feat_num': 9527, 'embed_dim': 8}, {'feat_name': 'I1', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I2', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I3', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I4', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I5', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I6', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I7', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I8', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I9', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I10', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I11', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I12', 'feat_num': 100, 'embed_dim': 8}, {'feat_name': 'I13', 'feat_num': 100, 'embed_dim': 8}]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from data_process.criteo import create_criteo_dataset\n",
    "\n",
    "import datetime\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../data/Criteo/train.txt'\n",
    "read_part = True\n",
    "sample_num = 5000000\n",
    "test_size = 0.2\n",
    "\n",
    "k = 8\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 4096\n",
    "epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns,(train_X,train_y),(test_X,test_y) = create_criteo_dataset(\"../data/Criteo/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 39)]              0         \n",
      "_________________________________________________________________\n",
      "fm__layer_1 (FM_Layer)       (None, 1)                 2183743   \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sigmoid_1 (Tenso [(None, 1)]               0         \n",
      "=================================================================\n",
      "Total params: 2,183,743\n",
      "Trainable params: 2,183,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xindun/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000/72000 [==============================] - 1s 15us/sample - loss: 0.6193 - auc_1: 0.5365 - val_loss: 0.5549 - val_auc_1: 0.5848\n",
      "Epoch 2/20\n",
      "72000/72000 [==============================] - 0s 2us/sample - loss: 0.5216 - auc_1: 0.6697 - val_loss: 0.5170 - val_auc_1: 0.6434\n",
      "Epoch 3/20\n",
      "72000/72000 [==============================] - 0s 2us/sample - loss: 0.4824 - auc_1: 0.7705 - val_loss: 0.4978 - val_auc_1: 0.6959\n",
      "Epoch 4/20\n",
      "72000/72000 [==============================] - 0s 2us/sample - loss: 0.4465 - auc_1: 0.8225 - val_loss: 0.4829 - val_auc_1: 0.7231\n",
      "Epoch 5/20\n",
      "72000/72000 [==============================] - 0s 2us/sample - loss: 0.4114 - auc_1: 0.8570 - val_loss: 0.4740 - val_auc_1: 0.7371\n",
      "Epoch 6/20\n",
      "72000/72000 [==============================] - 0s 2us/sample - loss: 0.3793 - auc_1: 0.8820 - val_loss: 0.4692 - val_auc_1: 0.7437\n",
      "Epoch 7/20\n",
      "72000/72000 [==============================] - 0s 2us/sample - loss: 0.3498 - auc_1: 0.9021 - val_loss: 0.4675 - val_auc_1: 0.7483\n",
      "Epoch 8/20\n",
      "72000/72000 [==============================] - 0s 2us/sample - loss: 0.3230 - auc_1: 0.9192 - val_loss: 0.4675 - val_auc_1: 0.7503\n",
      "Epoch 9/20\n",
      "72000/72000 [==============================] - 0s 2us/sample - loss: 0.2987 - auc_1: 0.9328 - val_loss: 0.4686 - val_auc_1: 0.7506\n",
      "时间为:2\n",
      "20000/20000 [==============================] - 0s 1us/sample - loss: 0.4679 - auc_1: 0.7561\n",
      "test AUC:  0.756077\n"
     ]
    }
   ],
   "source": [
    "model = FM(feature_columns=feature_columns, k=k)\n",
    "model.summary()\n",
    "model.compile(loss=binary_crossentropy,optimizer=Adam(learning_rate=learning_rate),metrics=[AUC()])\n",
    "\n",
    "t1 = datetime.datetime.now()\n",
    "model.fit(train_X,train_y,epochs=epochs,\n",
    "         callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2)],\n",
    "         batch_size=batch_size,\n",
    "          validation_split=0.1\n",
    "         ) \n",
    "t2 = datetime.datetime.now()\n",
    "print(\"时间为:%d\" %((t2-t1).total_seconds()))\n",
    "print(\"test AUC:  %f\" %model.evaluate(test_X,test_y,batch_size=batch_size)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
